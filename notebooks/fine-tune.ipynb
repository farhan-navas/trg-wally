{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# OpenAI Fine-Tuning API\n",
    "Detailed process of how I will be using OpenAI fine-tuning API for Wally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create instance of OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "For this section we will compile all our word doc datasets into jsonl files which the OpenAI API requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary conversion util scripts\n",
    "from scripts.docs.docx_to_json import convert_docx_folder_to_json\n",
    "from scripts.utils.json_to_jsonl import convert_to_jsonl\n",
    "\n",
    "# Word doc setup\n",
    "docx_folder_patterns = ['advice', 'dating-advice', 'relationship-advice', 'ask-singapore', 'work-advice']\n",
    "data_folder_path_cn = \"../data/processed-word-docs/chinese/reddit\"\n",
    "data_folder_path_el = \"../data/processed-word-docs/english/reddit\"\n",
    "\n",
    "output_cn = \"../data/processed/chinese\"\n",
    "output_el = \"../data/processed/english\"\n",
    "\n",
    "# Convert all relevant data into jsonl format\n",
    "for p in docx_folder_patterns:\n",
    "    cn_path = f\"{data_folder_path_cn}/{p}/{p}-*.docx\"\n",
    "    el_path = f\"{data_folder_path_el}/{p}/{p}-*.docx\"\n",
    "\n",
    "    cn_out = f\"{output_cn}/reddit/{p}.json\"\n",
    "    el_out = f\"{output_el}/reddit/{p}.json\"\n",
    "\n",
    "    convert_docx_folder_to_json(docx_folder_pattern=cn_path, output_json=cn_out)\n",
    "    convert_docx_folder_to_json(docx_folder_pattern=el_path, output_json=el_out)\n",
    "\n",
    "    convert_to_jsonl(input_path=cn_out, output_path=f\"{cn_out}l\")\n",
    "    convert_to_jsonl(input_path=el_out, output_path=f\"{el_out}l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one training file each for english and chinese\n",
    "from scripts.utils.combine_jsonl import merge_files\n",
    "\n",
    "fin_cn = f\"{output_cn}/reddit/*.jsonl\"\n",
    "fin_el = f\"{output_el}/reddit/*.jsonl\"\n",
    "\n",
    "fout_cn = f\"{output_cn}/run-one-chi.jsonl\"\n",
    "fout_el = f\"{output_el}/run-one-eng.jsonl\"\n",
    "\n",
    "merge_files(pattern=fin_cn, output_path=fout_cn)\n",
    "merge_files(pattern=fin_el, output_path=fout_el)\n",
    "training_files = [fout_cn, fout_el]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integrity Check\n",
    "\n",
    "Just to make sure that file does not receive validation errors during finetuning.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils.user_ending_lines import find_user_ending_lines\n",
    "\n",
    "for ff in training_files:\n",
    "    lines = find_user_ending_lines(ff)\n",
    "    print(f\"lines ending with 'user' for {ff.split(\"/\")[-1]} are: {lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload files\n",
    "Use `client.files.create()` method from OpenAI Files API to upload training file (for now only training, no validation) to OpenAI API. Afterwards, store returned File object ID for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_ids = []\n",
    "for ff in training_files:\n",
    "    training_file = client.files.create(\n",
    "        file=open(ff, \"rb\"),\n",
    "        purpose=\"fine-tune\",\n",
    "    )\n",
    "\n",
    "    training_file_ids.append(training_file.id)\n",
    "    print(f\"Training file ID for {ff.split(\"/\")[-1]}: {training_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fine-tuning job\n",
    "Use the `client.fine_tuning.jobs.create()` method to create a fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = []\n",
    "for idx, id in enumerate(training_file_ids):\n",
    "    job = client.fine_tuning.jobs.create(\n",
    "        training_file=id,\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        suffix=\"wally\",\n",
    "    )\n",
    "\n",
    "    job_ids.append(job.id)\n",
    "    print(f\"Job ID for {training_files[idx].split(\"/\")[-1]}: {job.id}\")\n",
    "    print(f\"Job status for {training_files[idx].split(\"/\")[-1]}: {job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Job Status\n",
    "Check status using `client.fine_tuning.jobs.retrieve() ` method, which takes in job ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, id in enumerate(job_ids):\n",
    "    retrieve_response = client.fine_tuning.jobs.retrieve(id)\n",
    "\n",
    "    print(f\"Job ID: {retrieve_response.id}\")\n",
    "    print(f\"Job status: {retrieve_response.status}\")\n",
    "    print(f\"Model: {retrieve_response.model}\")\n",
    "    print(f\"Trained Tokens: {retrieve_response.trained_tokens} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List events of the job using the `client.fine_tuning.jobs.list_events()` method. Returns a list of events associated with the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, id in enumerate(job_ids):\n",
    "    response = client.fine_tuning.jobs.list_events(id)\n",
    "\n",
    "    events = response.data\n",
    "    events.reverse()\n",
    "\n",
    "    for event in events:\n",
    "        print(event.message)\n",
    "\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
